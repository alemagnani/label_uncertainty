# evaluate_amazon_models.py

import pandas as pd
import numpy as np
from scipy.special import digamma
from scipy.stats import wasserstein_distance
import random
from tqdm import tqdm # Use standard tqdm if not in notebook
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
import warnings

# ML Imports
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)
pd.options.mode.chained_assignment = None

print("--- Model Evaluation Script ---")

# --- Configuration ---
OUTPUT_DIR = '/home/alessandro/Documents/amazon_review/' # Adjusted path assuming models are saved here
MODEL_A_PATH = os.path.join(OUTPUT_DIR, 'model_A.pth')
MODEL_B_PATH = os.path.join(OUTPUT_DIR, 'model_B.pth')
VECTORIZER_PATH = os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.joblib')
AGG_DATA_CACHE = os.path.join(OUTPUT_DIR, 'aggregated_sampled_data.pkl')

# Training parameters (must match training script)
SAMPLE_SIZE = 20000
MIN_REVIEWS = 5
TEST_SPLIT_SIZE = 0.15
VALID_SPLIT_SIZE = 0.10
MAX_FEATURES = 10000

# Model & Evaluation
INPUT_DIM = MAX_FEATURES
HIDDEN_DIM = 128
OUTPUT_DIM = 5
BATCH_SIZE = 128

# Metric Parameters
N_MONTE_CARLO_EMD = 1000
EPSILON = 1e-9

# --- START: Configuration for n-bin analysis ---
N_BINS_CONFIG = [
    (MIN_REVIEWS, 5),
    (6, 10),
    (11, 20),
    (21, 50),
    (51, 100),
    (101, float('inf')) # Use infinity for the last bin boundary
]
# --- END: Configuration for n-bin analysis ---


DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# Ordinal mapping and K
STAR_RATINGS = [1, 2, 3, 4, 5]
K = len(STAR_RATINGS)
ORDINAL_INDICES = np.arange(K)

# --- Check for Required Files ---
# (Code omitted for brevity - keep the check from the previous script)
required_files = [MODEL_A_PATH, MODEL_B_PATH, VECTORIZER_PATH, AGG_DATA_CACHE]
missing_files = [f for f in required_files if not os.path.exists(f)]
if missing_files:
    print("Error: Missing required files generated by the training script:")
    for f in missing_files:
        print(f" - {f}")
    print("Please run the training script first.")
    exit()


# --- Model Architecture Definition ---
# (Code omitted for brevity - keep MLP class from the previous script)
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
    def forward(self, x):
        x = self.fc1(x); x = self.relu(x); x = self.fc2(x)
        return x


# --- PyTorch Dataset for Evaluation ---
# (Code omitted for brevity - keep AmazonEvalDataset class)
class AmazonEvalDataset(Dataset):
    def __init__(self, features, product_ids):
        self.features = features; self.product_ids = product_ids
    def __len__(self): return self.features.shape[0]
    def __getitem__(self, idx):
        feat = torch.tensor(self.features[idx].todense()).float().squeeze(0)
        pid = self.product_ids[idx]
        return feat, pid

# --- Metric Implementations ---
# (Code omitted for brevity - keep metric functions)
DISTANCE_MATRIX = np.abs(ORDINAL_INDICES[:, None] - ORDINAL_INDICES[None, :]) / (K - 1)
def calculate_emd_1d(p, q):
    p = p / (p.sum() + EPSILON); q = q / (q.sum() + EPSILON)
    try:
        uw = np.maximum(0, p); uw /= uw.sum() + EPSILON
        vw = np.maximum(0, q); vw /= vw.sum() + EPSILON
        return wasserstein_distance(ORDINAL_INDICES, ORDINAL_INDICES, uw, vw)
    except Exception: return np.nan
def calculate_ce(p, q):
    p = p / (p.sum() + EPSILON); q = q / (q.sum() + EPSILON)
    log_q = np.log(q + EPSILON)
    if not np.all(np.isfinite(log_q)): return np.nan
    ce = -np.sum(p * log_q)
    return ce if np.isfinite(ce) else np.nan
ALPHA = np.ones(K); ALPHA_0 = ALPHA.sum()
def calculate_expected_ce(counts, n, q):
    pa = ALPHA + counts; pa0 = ALPHA_0 + n
    ep = pa / (pa0 + EPSILON)
    return calculate_ce(ep, q)
def calculate_expected_emd(counts, n, q, n_samples=N_MONTE_CARLO_EMD):
    pa = ALPHA + counts
    if np.any(pa <= 0): pa = np.maximum(pa, EPSILON)
    try: sps = np.random.dirichlet(pa, n_samples)
    except ValueError:
        pa0 = ALPHA_0 + n; ep = pa / (pa0 + EPSILON)
        return calculate_emd_1d(ep, q) # Fallback
    emds = [calculate_emd_1d(ps, q) for ps in sps]
    vemds = [s for s in emds if not np.isnan(s)]
    return np.mean(vemds) if vemds else np.nan

# --- Main Evaluation Execution ---

# 1. Load Aggregated & Sampled Data
# (Code omitted for brevity - keep data loading)
print(f"Loading aggregated data from cache: {AGG_DATA_CACHE}")
df_aggregated_sampled = pd.read_pickle(AGG_DATA_CACHE)
if len(df_aggregated_sampled) != SAMPLE_SIZE: # Adjust if needed
     print(f"Warning: Cached data size ({len(df_aggregated_sampled)}) differs from SAMPLE_SIZE ({SAMPLE_SIZE}). Adjusting...")
     if len(df_aggregated_sampled) < SAMPLE_SIZE: print("Using all data from cache.")
     elif len(df_aggregated_sampled) > SAMPLE_SIZE:
         df_aggregated_sampled = df_aggregated_sampled.sample(n=SAMPLE_SIZE, random_state=42); print(f"Resampled to {len(df_aggregated_sampled)} products.")


# 2. Replicate Train/Validation/Test Split
# (Code omitted for brevity - keep splitting logic)
print("Replicating train/validation/test split...")
df_temp, df_test = train_test_split(df_aggregated_sampled, test_size=TEST_SPLIT_SIZE, random_state=42)
relative_valid_size = VALID_SPLIT_SIZE / (1.0 - TEST_SPLIT_SIZE)
df_train, df_valid = train_test_split(df_temp, test_size=relative_valid_size, random_state=42)
print(f"Test set size: {len(df_test)}")
test_titles = df_test['title'].tolist(); test_product_ids = df_test['product_id'].tolist()
test_labels_dict = df_test.set_index('product_id').to_dict('index')


# 3. Load TF-IDF Vectorizer
# (Code omitted for brevity - keep vectorizer loading)
print(f"Loading TF-IDF vectorizer...")
vectorizer = joblib.load(VECTORIZER_PATH)

# 4. Transform Test Set Titles
# (Code omitted for brevity - keep transform logic)
print("Transforming test set titles...")
test_features = vectorizer.transform(test_titles)

# 5. Load Trained Models
# (Code omitted for brevity - keep model loading)
print("Loading trained models...")
model_A = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM).to(DEVICE)
model_B = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM).to(DEVICE)
model_A.load_state_dict(torch.load(MODEL_A_PATH, map_location=DEVICE))
model_B.load_state_dict(torch.load(MODEL_B_PATH, map_location=DEVICE))
model_A.eval(); model_B.eval()
print("Models loaded.")

# 6. Generate Predictions on Test Set
# (Code omitted for brevity - keep prediction generation)
print("Generating predictions...")
test_dataset = AmazonEvalDataset(test_features, test_product_ids)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)
predictions = {}
with torch.no_grad():
    for features_batch, pids_batch in tqdm(test_loader, desc="Predicting"):
        features_batch = features_batch.to(DEVICE)
        probs_a = torch.softmax(model_A(features_batch), dim=1).cpu().numpy()
        probs_b = torch.softmax(model_B(features_batch), dim=1).cpu().numpy()
        for i, pid in enumerate(pids_batch): predictions[pid] = {'q_a': probs_a[i], 'q_b': probs_b[i]}
print(f"Generated predictions for {len(predictions)} test products.")


# 7. Calculate Metrics
# (Code omitted for brevity - keep metric calculation loop)
print("Calculating evaluation metrics...")
results = []
for pid, preds in tqdm(predictions.items(), desc="Calculating Metrics"):
    if pid not in test_labels_dict: continue
    counts = test_labels_dict[pid]['counts']; n = test_labels_dict[pid]['n']
    p_emp = counts / (n + EPSILON); q_a = preds['q_a']; q_b = preds['q_b']
    q_a = q_a / (q_a.sum() + EPSILON); q_b = q_b / (q_b.sum() + EPSILON); p_emp = p_emp / (p_emp.sum() + EPSILON)
    instance_results = {'product_id': pid, 'n': n}
    instance_results['CE_emp_A'] = calculate_ce(p_emp, q_a)
    instance_results['EMD_emp_A'] = calculate_emd_1d(p_emp, q_a)
    instance_results['E_CE_A'] = calculate_expected_ce(counts, n, q_a)
    instance_results['E_EMD_A'] = calculate_expected_emd(counts, n, q_a)
    instance_results['CE_emp_B'] = calculate_ce(p_emp, q_b)
    instance_results['EMD_emp_B'] = calculate_emd_1d(p_emp, q_b)
    instance_results['E_CE_B'] = calculate_expected_ce(counts, n, q_b)
    instance_results['E_EMD_B'] = calculate_expected_emd(counts, n, q_b)
    results.append(instance_results)

df_results = pd.DataFrame(results)
df_results.dropna(inplace=True)

if df_results.empty:
        print("\nNo valid results after metric calculation. Cannot proceed.")
else:
    # --- START: n-bin Analysis ---

    # 9. Define Bins and Assign to Results
    print("\nPerforming analysis based on number of reviews (n)...")
    bin_labels = []
    for low, high in N_BINS_CONFIG:
        if high == float('inf'):
            bin_labels.append(f"{low}+")
        else:
            bin_labels.append(f"{low}-{high}")

    def assign_bin(n):
        for i, (low, high) in enumerate(N_BINS_CONFIG):
            if low <= n <= high:
                return bin_labels[i]
        return "Other" # Fallback

    df_results['n_bin'] = df_results['n'].apply(assign_bin)
    # Ensure bins are ordered correctly for plotting
    df_results['n_bin'] = pd.Categorical(df_results['n_bin'], categories=bin_labels, ordered=True)

    # 10. Calculate Bin Averages
    bin_analysis = df_results.groupby('n_bin').agg(
        Count=('product_id', 'count'),
        CE_emp_A_mean=('CE_emp_A', 'mean'), E_CE_A_mean=('E_CE_A', 'mean'),
        EMD_emp_A_mean=('EMD_emp_A', 'mean'), E_EMD_A_mean=('E_EMD_A', 'mean'),
        CE_emp_B_mean=('CE_emp_B', 'mean'), E_CE_B_mean=('E_CE_B', 'mean'),
        EMD_emp_B_mean=('EMD_emp_B', 'mean'), E_EMD_B_mean=('E_EMD_B', 'mean')
    ).reset_index()

    print("\n--- Average Scores per Number of Reviews (n) Bin ---")
    print(bin_analysis.round(4).to_markdown(index=False, numalign="right", stralign="right"))

    # 11. Visualize Bin Results
    # Prepare data for plotting (melt format)
    df_plot_n = bin_analysis.melt(id_vars=['n_bin', 'Count'], var_name='Metric_Model', value_name='Average Score')
    df_plot_n[['Metric_Type', 'Model', '_']] = df_plot_n['Metric_Model'].str.split('_', expand=True)
    df_plot_n['Model_Name'] = df_plot_n['Model'].map({'A': 'Model A (CE Loss)', 'B': 'Model B (KL Loss - Smooth)'}) # Map for legend
    # Map metric types for plotting separation (Empirical vs Expected)
    df_plot_n['Metric_Base'] = df_plot_n['Metric_Type'].replace({'CEemp': 'CE', 'ECE': 'CE', 'EMDemp': 'EMD', 'EEMD': 'EMD'})
    df_plot_n['Metric_Kind'] = df_plot_n['Metric_Type'].apply(lambda x: 'Expected' if x.startswith('E') else 'Empirical')

    # Plot 1: Compare Models within each metric type across n_bins
    metric_types_plot = ['CE_emp', 'E_CE', 'EMD_emp', 'E_EMD']
    metric_names_map = {'CE_emp':'CE Empirical', 'E_CE': 'E[CE]', 'EMD_emp': 'EMD Empirical', 'E_EMD': 'E[EMD]'}

    fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharex=True)
    axes = axes.flatten()
    fig.suptitle('Model Comparison Across Review Count Bins (n)', fontsize=16, y=1.03)

    for i, metric_key in enumerate(metric_types_plot):
        metric_full_name = metric_names_map[metric_key]
        df_metric_bin = df_plot_n[df_plot_n['Metric_Model'].str.startswith(metric_key)]
        sns.pointplot(data=df_metric_bin, x='n_bin', y='Average Score', hue='Model_Name', palette='viridis', ax=axes[i], markers=["o", "s"], linestyles=["--", "-"])
        axes[i].set_title(f'Metric: {metric_full_name}')
        axes[i].set_ylabel('Avg Score (Lower is Better)')
        axes[i].set_xlabel('')
        axes[i].tick_params(axis='x', rotation=45)
        axes[i].grid(True, axis='y', linestyle=':', alpha=0.7)
        # Annotate count per bin
        if i >= 2 : # Add x-label to bottom plots only
             axes[i].set_xlabel('Number of Reviews (n)')
        for j, bin_label in enumerate(bin_labels):
            count = bin_analysis.loc[bin_analysis['n_bin'] == bin_label, 'Count'].iloc[0]
            axes[i].text(j, axes[i].get_ylim()[1]*0.95, f"N={count}", ha='center', va='top', fontsize=8, color='gray')
        axes[i].legend(title='Model', fontsize=9)


    plt.tight_layout(rect=[0, 0, 1, 0.97]) # Adjust layout
    plt.show()

    # Plot 2: Compare Empirical vs Expected for each metric base across n_bins (Average over models for simplicity or show both)
    fig, axes = plt.subplots(1, 2, figsize=(14, 5.5), sharex=True)
    fig.suptitle('Empirical vs. Expected Metrics Across Review Count Bins (n)', fontsize=16, y=1.03)

    # CE Comparison
    df_ce_bin = df_plot_n[df_plot_n['Metric_Base'] == 'CE']
    sns.pointplot(data=df_ce_bin, x='n_bin', y='Average Score', hue='Metric_Kind', palette='rocket', ax=axes[0], markers=["o", "s"], linestyles=["--", "-"], errorbar=('ci', 95))
    axes[0].set_title('Cross-Entropy (CE vs E[CE])')
    axes[0].set_ylabel('Avg Score (Lower is Better)')
    axes[0].set_xlabel('Number of Reviews (n)')
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].grid(True, axis='y', linestyle=':', alpha=0.7)
    axes[0].legend(title='Metric Kind', fontsize=9)

    # EMD Comparison
    df_emd_bin = df_plot_n[df_plot_n['Metric_Base'] == 'EMD']
    sns.pointplot(data=df_emd_bin, x='n_bin', y='Average Score', hue='Metric_Kind', palette='mako', ax=axes[1], markers=["o", "s"], linestyles=["--", "-"], errorbar=('ci', 95))
    axes[1].set_title('Earth Mover\'s Distance (EMD vs E[EMD])')
    axes[1].set_ylabel('Avg Score (Lower is Better)')
    axes[1].set_xlabel('Number of Reviews (n)')
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].grid(True, axis='y', linestyle=':', alpha=0.7)
    axes[1].legend(title='Metric Kind', fontsize=9)

    plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust layout
    plt.show()


    print("\n--- Interpretation Notes (n-bin analysis) ---")
    print("*   Plot 1 (Model Comparison): Does the winning model (A vs B) change in low-n bins compared to high-n bins?")
    print("    - Does E[EMD] show a consistent advantage for one model across bins, even if CE metrics fluctuate?")
    print("*   Plot 2 (Empirical vs Expected):")
    print("    - Is the gap between Empirical and Expected scores largest for the lowest 'n' bin?")
    print("    - Do the Expected metric lines appear smoother or show less variance (tighter CI if shown) across bins compared to Empirical?")


    # --- Keep the Overall Ranking Section ---
    print(f"\nCalculating overall average scores over {len(df_results)} test set products...")
    avg_scores = df_results.mean(numeric_only=True)
    # ... (rest of the ranking table and overall bar plot code remains the same) ...
    ranking_data = {
        'Metric': ['CE_emp', 'E[CE]', 'EMD_emp', 'E[EMD]'],
        'Model A (CE Loss)': [ avg_scores.get(k, np.nan) for k in ['CE_emp_A', 'E_CE_A', 'EMD_emp_A', 'E_EMD_A']],
        'Model B (KL Loss - Smooth)': [ avg_scores.get(k, np.nan) for k in ['CE_emp_B', 'E_CE_B', 'EMD_emp_B', 'E_EMD_B']]
    }
    df_ranking = pd.DataFrame(ranking_data).set_index('Metric')
    df_ranking['Winner'] = df_ranking.idxmin(axis=1, skipna=True)
    print("\n--- Overall Model Ranking (Test Set Evaluation) ---")
    print(df_ranking.round(4).to_markdown(numalign="right", stralign="right"))
    df_plot_overall = df_ranking.drop(columns=['Winner']).reset_index().melt(id_vars='Metric', var_name='Model', value_name='Average Score')
    plt.figure(figsize=(10, 6))
    ax = sns.barplot(data=df_plot_overall, x='Metric', y='Average Score', hue='Model', palette='viridis')
    plt.title('Overall Comparison of Trained Model Performance - Amazon Electronics Ratings\n(Test Set Evaluation)')
    plt.ylabel('Average Metric Score (Lower is Better)')
    plt.xlabel('Evaluation Metric')
    plt.xticks(rotation=0)
    for container in ax.containers: ax.bar_label(container, fmt='%.3f', padding=3)
    plt.ylim(bottom=0); plt.tight_layout(); plt.show()
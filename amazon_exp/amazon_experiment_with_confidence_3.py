# evaluate_amazon_models_with_ci.py

import pandas as pd
import numpy as np
from scipy.special import digamma
from scipy.stats import wasserstein_distance, ttest_rel
import random
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
import warnings
import matplotlib as mpl
from scipy import stats

# ML Imports
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import TransformerEncoder, TransformerEncoderLayer
from torch.utils.data import Dataset, DataLoader
import torch.distributions as D

# Configure matplotlib for publication-quality plots
mpl.rcParams.update({
    'font.size': 12,
    'axes.labelsize': 12,
    'axes.titlesize': 14,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'figure.titlesize': 16,
    'figure.figsize': (8, 6),
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.1,
    'figure.autolayout': True,
    'axes.grid': True,
    'grid.alpha': 0.3,
})

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)
pd.options.mode.chained_assignment = None

print("--- Model Evaluation Script with Confidence Intervals ---")

# --- Configuration ---
OUTPUT_DIR = '/home/alessandro/Documents/amazon_review/'  # Adjusted path assuming models are saved here
MODEL_A_PATH = os.path.join(OUTPUT_DIR, 'model_A.pth')
MODEL_B_PATH = os.path.join(OUTPUT_DIR, 'model_B.pth')
MODEL_C_PATH = os.path.join(OUTPUT_DIR, 'model_C.pth')
MODEL_D_PATH = os.path.join(OUTPUT_DIR, 'model_D.pth')
VECTORIZER_PATH = os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.joblib')
AGG_DATA_CACHE = os.path.join(OUTPUT_DIR, 'aggregated_sampled_data.pkl')

# Training parameters (must match training script)
SAMPLE_SIZE = 20000
MIN_REVIEWS = 5
TEST_SPLIT_SIZE = 0.15
VALID_SPLIT_SIZE = 0.10
MAX_FEATURES = 10000

# Model & Evaluation
INPUT_DIM = MAX_FEATURES
HIDDEN_DIM = 128
OUTPUT_DIM = 5
BATCH_SIZE = 128
DROPOUT_RATE = 0.2
TRANSFORMER_NHEAD = 4
TRANSFORMER_LAYERS = 2

# Metric Parameters
N_MONTE_CARLO_EMD = 1000
EPSILON = 1e-9

# Confidence Interval Parameters
N_BOOTSTRAP = 1000
CONFIDENCE_LEVEL = 0.95

# --- Configuration for n-bin analysis ---
N_BINS_CONFIG = [
    (MIN_REVIEWS, 5),
    (6, 10),
    (11, 20),
    (21, 50),
    (51, 100),
    (101, float('inf'))  # Use infinity for the last bin boundary
]

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# Ordinal mapping and K
STAR_RATINGS = [1, 2, 3, 4, 5]
K = len(STAR_RATINGS)
ORDINAL_INDICES = np.arange(K)


# --- Bootstrap Confidence Interval function ---
def bootstrap_confidence_interval(data, n_bootstrap=N_BOOTSTRAP, confidence=CONFIDENCE_LEVEL):
    """Calculate bootstrap confidence interval for a metric."""
    if len(data) <= 1:
        return np.nan, np.nan

    bootstrap_samples = []
    for _ in range(n_bootstrap):
        # Sample with replacement
        idx = np.random.choice(len(data), len(data), replace=True)
        sample = data[idx]
        bootstrap_samples.append(np.nanmean(sample))

    # Calculate confidence interval
    alpha = (1 - confidence) / 2
    lower = np.percentile(bootstrap_samples, alpha * 100)
    upper = np.percentile(bootstrap_samples, (1 - alpha) * 100)

    return lower, upper


# Function to calculate statistical significance
def calculate_significance(df, model_a, model_b, metric):
    """Calculate if the difference between models is statistically significant."""
    col_a = f"{metric}_{model_a}"
    col_b = f"{metric}_{model_b}"

    # Remove NaN values (paired samples)
    mask = ~np.isnan(df[col_a]) & ~np.isnan(df[col_b])
    if np.sum(mask) < 2:
        return False, 1.0

    # Paired t-test since same test instances
    t_stat, p_value = ttest_rel(df[col_a][mask], df[col_b][mask])

    return p_value < 0.05, p_value


# --- Check for Required Files ---
required_files = [MODEL_A_PATH, MODEL_B_PATH, MODEL_C_PATH, MODEL_D_PATH, VECTORIZER_PATH, AGG_DATA_CACHE]
missing_files = [f for f in required_files if not os.path.exists(f)]
if missing_files:
    print("Error: Missing required files generated by the training script:")
    for f in missing_files:
        print(f" - {f}")
    print("Please run the training script first.")
    exit()


# --- Model Architecture Definitions ---
# Model A & B: Simple MLP
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x


# Model C: Bayesian Ordinal Neural Network
class BONN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
        # Use a single output for ordinal prediction (cumulative approach)
        self.theta = nn.Parameter(torch.zeros(output_dim - 1))
        self.beta = nn.Linear(hidden_dim // 2, 1)
        self.output_dim = output_dim

    def forward(self, x):
        # Feature extraction
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = F.relu(x)

        # Projection to scalar
        f = self.beta(x).squeeze(-1)

        # Ensure thresholds are ordered (cumulative approach)
        theta = torch.cat([
            torch.tensor([-float('inf')], device=self.theta.device),
            torch.cumsum(F.softplus(self.theta), dim=0),
            torch.tensor([float('inf')], device=self.theta.device)
        ])

        # Compute probabilities for each ordinal class
        probs = []
        for i in range(self.output_dim):
            probs.append(torch.sigmoid(theta[i + 1] - f) - torch.sigmoid(theta[i] - f))

        return torch.stack(probs, dim=1)


# Model D: Transformer-Based Ordinal Rating Predictor
class TORP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, nhead=4, num_layers=2, dropout=0.1):
        super().__init__()
        # Initial projection from sparse to dense
        self.input_proj = nn.Linear(input_dim, hidden_dim)

        # Transformer encoder
        encoder_layers = TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=nhead,
            dim_feedforward=hidden_dim * 2,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)

        # Global attention pooling
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )

        # Output layer
        self.fc_out = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # Create dense embedding from sparse input
        x = self.input_proj(x)
        x = F.relu(x)

        # Reshape for transformer: [batch, seq_len=1, hidden_dim]
        x = x.unsqueeze(1)

        # Apply transformer encoder
        x = self.transformer_encoder(x)

        # Global attention pooling
        attn_weights = F.softmax(self.attention(x), dim=1)
        x = torch.sum(x * attn_weights, dim=1)

        # Output layer
        logits = self.fc_out(x)

        return F.softmax(logits, dim=1)  # Direct probability output


# --- PyTorch Dataset for Evaluation ---
class AmazonEvalDataset(Dataset):
    def __init__(self, features, product_ids):
        self.features = features
        self.product_ids = product_ids

    def __len__(self):
        return self.features.shape[0]

    def __getitem__(self, idx):
        feat = torch.tensor(self.features[idx].todense()).float().squeeze(0)
        pid = self.product_ids[idx]
        return feat, pid


# --- Metric Implementations ---
DISTANCE_MATRIX = np.abs(ORDINAL_INDICES[:, None] - ORDINAL_INDICES[None, :]) / (K - 1)


def calculate_emd_1d(p, q):
    p = p / (p.sum() + EPSILON)
    q = q / (q.sum() + EPSILON)
    try:
        uw = np.maximum(0, p)
        uw /= uw.sum() + EPSILON
        vw = np.maximum(0, q)
        vw /= vw.sum() + EPSILON
        return wasserstein_distance(ORDINAL_INDICES, ORDINAL_INDICES, uw, vw)
    except Exception:
        return np.nan


def calculate_ce(p, q):
    p = p / (p.sum() + EPSILON)
    q = q / (q.sum() + EPSILON)
    log_q = np.log(q + EPSILON)
    if not np.all(np.isfinite(log_q)):
        return np.nan
    ce = -np.sum(p * log_q)
    return ce if np.isfinite(ce) else np.nan


ALPHA = np.ones(K)
ALPHA_0 = ALPHA.sum()


def calculate_expected_ce(counts, n, q):
    pa = ALPHA + counts
    pa0 = ALPHA_0 + n
    ep = pa / (pa0 + EPSILON)
    return calculate_ce(ep, q)


def calculate_expected_emd(counts, n, q, n_samples=N_MONTE_CARLO_EMD):
    pa = ALPHA + counts
    if np.any(pa <= 0):
        pa = np.maximum(pa, EPSILON)
    try:
        sps = np.random.dirichlet(pa, n_samples)
    except ValueError:
        pa0 = ALPHA_0 + n
        ep = pa / (pa0 + EPSILON)
        return calculate_emd_1d(ep, q)  # Fallback
    emds = [calculate_emd_1d(ps, q) for ps in sps]
    vemds = [s for s in emds if not np.isnan(s)]
    return np.mean(vemds) if vemds else np.nan


# --- Main Evaluation Execution ---

# 1. Load Aggregated & Sampled Data
print(f"Loading aggregated data from cache: {AGG_DATA_CACHE}")
df_aggregated_sampled = pd.read_pickle(AGG_DATA_CACHE)
if len(df_aggregated_sampled) != SAMPLE_SIZE:
    print(
        f"Warning: Cached data size ({len(df_aggregated_sampled)}) differs from SAMPLE_SIZE ({SAMPLE_SIZE}). Adjusting...")
    if len(df_aggregated_sampled) < SAMPLE_SIZE:
        print("Using all data from cache.")
    elif len(df_aggregated_sampled) > SAMPLE_SIZE:
        df_aggregated_sampled = df_aggregated_sampled.sample(n=SAMPLE_SIZE, random_state=42)
        print(f"Resampled to {len(df_aggregated_sampled)} products.")

# 2. Replicate Train/Validation/Test Split
print("Replicating train/validation/test split...")
df_temp, df_test = train_test_split(df_aggregated_sampled, test_size=TEST_SPLIT_SIZE, random_state=42)
relative_valid_size = VALID_SPLIT_SIZE / (1.0 - TEST_SPLIT_SIZE)
df_train, df_valid = train_test_split(df_temp, test_size=relative_valid_size, random_state=42)
print(f"Test set size: {len(df_test)}")
test_titles = df_test['title'].tolist()
test_product_ids = df_test['product_id'].tolist()
test_labels_dict = df_test.set_index('product_id').to_dict('index')

# 3. Load TF-IDF Vectorizer
print(f"Loading TF-IDF vectorizer...")
vectorizer = joblib.load(VECTORIZER_PATH)

# 4. Transform Test Set Titles
print("Transforming test set titles...")
test_features = vectorizer.transform(test_titles)

# 5. Load Trained Models
print("Loading trained models...")
model_A = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM).to(DEVICE)
model_B = MLP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM).to(DEVICE)
model_C = BONN(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, dropout_rate=DROPOUT_RATE).to(DEVICE)
model_D = TORP(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM,
               nhead=TRANSFORMER_NHEAD,
               num_layers=TRANSFORMER_LAYERS,
               dropout=DROPOUT_RATE).to(DEVICE)

model_A.load_state_dict(torch.load(MODEL_A_PATH, map_location=DEVICE))
model_B.load_state_dict(torch.load(MODEL_B_PATH, map_location=DEVICE))
model_C.load_state_dict(torch.load(MODEL_C_PATH, map_location=DEVICE))
model_D.load_state_dict(torch.load(MODEL_D_PATH, map_location=DEVICE))

model_A.eval()
model_B.eval()
model_C.eval()
model_D.eval()
print("All models loaded.")

# 6. Generate Predictions on Test Set
print("Generating predictions...")
test_dataset = AmazonEvalDataset(test_features, test_product_ids)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)
predictions = {}
with torch.no_grad():
    for features_batch, pids_batch in tqdm(test_loader, desc="Predicting"):
        features_batch = features_batch.to(DEVICE)
        # Model A & B - need to apply softmax to logits
        probs_a = torch.softmax(model_A(features_batch), dim=1).cpu().numpy()
        probs_b = torch.softmax(model_B(features_batch), dim=1).cpu().numpy()
        # Model C & D - already output probabilities
        probs_c = model_C(features_batch).cpu().numpy()
        probs_d = model_D(features_batch).cpu().numpy()

        for i, pid in enumerate(pids_batch):
            predictions[pid] = {
                'q_a': probs_a[i],
                'q_b': probs_b[i],
                'q_c': probs_c[i],
                'q_d': probs_d[i]
            }
print(f"Generated predictions for {len(predictions)} test products.")

# 7. Calculate Metrics
print("Calculating evaluation metrics...")
results = []
for pid, preds in tqdm(predictions.items(), desc="Calculating Metrics"):
    if pid not in test_labels_dict:
        continue

    counts = test_labels_dict[pid]['counts']
    n = test_labels_dict[pid]['n']
    p_emp = counts / (n + EPSILON)

    q_a = preds['q_a'] / (preds['q_a'].sum() + EPSILON)
    q_b = preds['q_b'] / (preds['q_b'].sum() + EPSILON)
    q_c = preds['q_c'] / (preds['q_c'].sum() + EPSILON)
    q_d = preds['q_d'] / (preds['q_d'].sum() + EPSILON)
    p_emp = p_emp / (p_emp.sum() + EPSILON)

    instance_results = {'product_id': pid, 'n': n}

    # Model A metrics
    instance_results['CE_emp_A'] = calculate_ce(p_emp, q_a)
    instance_results['EMD_emp_A'] = calculate_emd_1d(p_emp, q_a)
    instance_results['E_CE_A'] = calculate_expected_ce(counts, n, q_a)
    instance_results['E_EMD_A'] = calculate_expected_emd(counts, n, q_a)

    # Model B metrics
    instance_results['CE_emp_B'] = calculate_ce(p_emp, q_b)
    instance_results['EMD_emp_B'] = calculate_emd_1d(p_emp, q_b)
    instance_results['E_CE_B'] = calculate_expected_ce(counts, n, q_b)
    instance_results['E_EMD_B'] = calculate_expected_emd(counts, n, q_b)

    # Model C metrics
    instance_results['CE_emp_C'] = calculate_ce(p_emp, q_c)
    instance_results['EMD_emp_C'] = calculate_emd_1d(p_emp, q_c)
    instance_results['E_CE_C'] = calculate_expected_ce(counts, n, q_c)
    instance_results['E_EMD_C'] = calculate_expected_emd(counts, n, q_c)

    # Model D metrics
    instance_results['CE_emp_D'] = calculate_ce(p_emp, q_d)
    instance_results['EMD_emp_D'] = calculate_emd_1d(p_emp, q_d)
    instance_results['E_CE_D'] = calculate_expected_ce(counts, n, q_d)
    instance_results['E_EMD_D'] = calculate_expected_emd(counts, n, q_d)

    results.append(instance_results)

df_results = pd.DataFrame(results)
df_results.dropna(inplace=True)

if df_results.empty:
    print("\nNo valid results after metric calculation. Cannot proceed.")
else:
    # --- START: n-bin Analysis ---

    # 9. Define Bins and Assign to Results
    print("\nPerforming analysis based on number of reviews (n)...")
    bin_labels = []
    for low, high in N_BINS_CONFIG:
        if high == float('inf'):
            bin_labels.append(f"{low}+")
        else:
            bin_labels.append(f"{low}-{high}")


    def assign_bin(n):
        for i, (low, high) in enumerate(N_BINS_CONFIG):
            if low <= n <= high:
                return bin_labels[i]
        return "Other"  # Fallback


    df_results['n_bin'] = df_results['n'].apply(assign_bin)
    # Ensure bins are ordered correctly for plotting
    df_results['n_bin'] = pd.Categorical(df_results['n_bin'], categories=bin_labels, ordered=True)

    # 10. Calculate Bin Averages with Confidence Intervals
    models_list = ["A", "B", "C", "D"]
    model_names = {
        "A": "Model A (MLP+CE)",
        "B": "Model B (MLP+KL)",
        "C": "Model C (BONN)",
        "D": "Model D (TORP+EMD)"
    }

    metric_types = ["CE_emp", "E_CE", "EMD_emp", "E_EMD"]
    metric_names = {
        "CE_emp": "CE Empirical",
        "E_CE": "E[CE]",
        "EMD_emp": "EMD Empirical",
        "E_EMD": "E[EMD]"
    }

    bin_analysis_with_ci = []

    for bin_name in bin_labels:
        bin_data = df_results[df_results['n_bin'] == bin_name]
        bin_count = len(bin_data)

        # For each model and metric
        for model in models_list:
            for metric in metric_types:
                metric_col = f"{metric}_{model}"
                values = bin_data[metric_col].values

                # Calculate mean
                mean_value = np.nanmean(values)

                # Calculate 95% confidence interval
                lower, upper = bootstrap_confidence_interval(values)

                bin_analysis_with_ci.append({
                    'n_bin': bin_name,
                    'Count': bin_count,
                    'Model': model_names[model],
                    'Metric': metric_names[metric],
                    'Mean': mean_value,
                    'Lower': lower,
                    'Upper': upper
                })

    df_ci = pd.DataFrame(bin_analysis_with_ci)

    # For the summary table without CI
    bin_analysis = df_results.groupby('n_bin', observed=False).agg(
        Count=('product_id', 'count'),
        # Model A
        CE_emp_A_mean=('CE_emp_A', 'mean'),
        E_CE_A_mean=('E_CE_A', 'mean'),
        EMD_emp_A_mean=('EMD_emp_A', 'mean'),
        E_EMD_A_mean=('E_EMD_A', 'mean'),
        # Model B
        CE_emp_B_mean=('CE_emp_B', 'mean'),
        E_CE_B_mean=('E_CE_B', 'mean'),
        EMD_emp_B_mean=('EMD_emp_B', 'mean'),
        E_EMD_B_mean=('E_EMD_B', 'mean'),
        # Model C
        CE_emp_C_mean=('CE_emp_C', 'mean'),
        E_CE_C_mean=('E_CE_C', 'mean'),
        EMD_emp_C_mean=('EMD_emp_C', 'mean'),
        E_EMD_C_mean=('E_EMD_C', 'mean'),
        # Model D
        CE_emp_D_mean=('CE_emp_D', 'mean'),
        E_CE_D_mean=('E_CE_D', 'mean'),
        EMD_emp_D_mean=('EMD_emp_D', 'mean'),
        E_EMD_D_mean=('E_EMD_D', 'mean')
    ).reset_index()

    print("\n--- Average Scores per Number of Reviews (n) Bin ---")
    print(bin_analysis.round(4).to_markdown(index=False, numalign="right", stralign="right"))

    # 11. Calculate Statistical Significance
    print("\nCalculating statistical significance between models...")
    significance_results = []

    # Compare all model pairs for E[EMD] metric
    for bin_name in bin_labels:
        bin_data = df_results[df_results['n_bin'] == bin_name]

        for i, model_i in enumerate(models_list):
            for j, model_j in enumerate(models_list):
                if i >= j:  # Skip self-comparisons and duplicate pairs
                    continue

                for metric in metric_types:
                    is_significant, p_value = calculate_significance(bin_data, model_i, model_j, metric)

                    significance_results.append({
                        'n_bin': bin_name,
                        'ModelA': model_names[model_i],
                        'ModelB': model_names[model_j],
                        'Metric': metric_names[metric],
                        'Significant': is_significant,
                        'p_value': p_value
                    })

    df_significance = pd.DataFrame(significance_results)

    # Extract significant differences for E[EMD] for reporting
    emd_significance = df_significance[df_significance['Metric'] == 'E[EMD]'].copy()
    emd_significance['Comparison'] = emd_significance.apply(
        lambda x: f"{x['ModelA']} vs {x['ModelB']}", axis=1
    )

    print("\n--- Significant Differences in E[EMD] (p < 0.05) ---")
    sig_results = emd_significance[emd_significance['Significant']].sort_values(['n_bin', 'p_value'])
    print(sig_results[['n_bin', 'Comparison', 'p_value']].round(4).to_markdown(index=False))

    # 12. Create visualizations with confidence intervals
    # For the E[EMD] bin comparison plot
    plt.figure(figsize=(10, 6))
    df_metric = df_ci[df_ci['Metric'] == 'E[EMD]']

    for model_name in model_names.values():
        df_model = df_metric[df_metric['Model'] == model_name]

        plt.errorbar(
            x=df_model['n_bin'],
            y=df_model['Mean'],
            yerr=[df_model['Mean'] - df_model['Lower'], df_model['Upper'] - df_model['Mean']],
            fmt='-o',
            capsize=5,
            label=model_name
        )

    plt.title('Expected Earth Mover\'s Distance (E[EMD]) Across Review Count Bins')
    plt.ylabel('Average Score (Lower is Better)')
    plt.xlabel('Number of Reviews (n)')
    plt.xticks(rotation=45)
    plt.grid(True, axis='y', linestyle=':', alpha=0.7)
    plt.legend(title='Model')
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'model_comparison_E_EMD_with_ci.png'), bbox_inches='tight', dpi=300)
    plt.savefig(os.path.join(OUTPUT_DIR, 'model_comparison_E_EMD_with_ci.pdf'), bbox_inches='tight', dpi=300)
    plt.close()

    # For the CE Empirical bin comparison plot
    plt.figure(figsize=(10, 6))
    df_metric = df_ci[df_ci['Metric'] == 'CE Empirical']

    for model_name in model_names.values():
        df_model = df_metric[df_metric['Model'] == model_name]

        plt.errorbar(
            x=df_model['n_bin'],
            y=df_model['Mean'],
            yerr=[df_model['Mean'] - df_model['Lower'], df_model['Upper'] - df_model['Mean']],
            fmt='-o',
            capsize=5,
            label=model_name
        )

    plt.title('CE Empirical Across Review Count Bins')
    plt.ylabel('Average Score (Lower is Better)')
    plt.xlabel('Number of Reviews (n)')
    plt.xticks(rotation=45)
    plt.grid(True, axis='y', linestyle=':', alpha=0.7)
    plt.legend(title='Model')
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'model_comparison_CE_emp_with_ci.png'), bbox_inches='tight', dpi=300)
    plt.savefig(os.path.join(OUTPUT_DIR, 'model_comparison_CE_emp_with_ci.pdf'), bbox_inches='tight', dpi=300)
    plt.close()

    # Overall model comparison with confidence intervals
    print(f"\nCalculating overall average scores over {len(df_results)} test set products...")

    # Calculate overall metrics with confidence intervals
    overall_with_ci = []

    for model in models_list:
        for metric in metric_types:
            metric_col = f"{metric}_{model}"
            values = df_results[metric_col].values

            # Calculate mean
            mean_value = np.nanmean(values)

            # Calculate 95% confidence interval
            lower, upper = bootstrap_confidence_interval(values)

            overall_with_ci.append({
                'Model': model_names[model],
                'Metric': metric_names[metric],
                'Mean': mean_value,
                'Lower': lower,
                'Upper': upper
            })

    df_overall_ci = pd.DataFrame(overall_with_ci)

    # Calculate significance for overall metrics
    overall_significance = []

    for i, model_i in enumerate(models_list):
        for j, model_j in enumerate(models_list):
            if i >= j:  # Skip self-comparisons and duplicate pairs
                continue

            for metric in metric_types:
                is_significant, p_value = calculate_significance(df_results, model_i, model_j, metric)

                overall_significance.append({
                    'ModelA': model_names[model_i],
                    'ModelB': model_names[model_j],
                    'Metric': metric_names[metric],
                    'Significant': is_significant,
                    'p_value': p_value
                })

    df_overall_sig = pd.DataFrame(overall_significance)

    # Create overall comparison plot with confidence intervals
    plt.figure(figsize=(12, 8))

    # Group by metric for easier plotting
    for i, metric in enumerate(metric_names.values()):
        # Create overall comparison plot with confidence intervals
        plt.figure(figsize=(12, 8))

        # Group by metric for easier plotting
        for i, metric in enumerate(metric_names.values()):
            plt.subplot(2, 2, i + 1)

            df_metric = df_overall_ci[df_overall_ci['Metric'] == metric]
            x_pos = np.arange(len(model_names))

            # Plot bars with error bars
            bars = plt.bar(
                x_pos,
                df_metric['Mean'],
                yerr=[df_metric['Mean'] - df_metric['Lower'], df_metric['Upper'] - df_metric['Mean']],
                capsize=5,
                alpha=0.7
            )

            # Add value labels
            for j, bar in enumerate(bars):
                plt.text(
                    bar.get_x() + bar.get_width() / 2,
                    bar.get_height() + 0.01,
                    f"{df_metric.iloc[j]['Mean']:.3f}",
                    ha='center',
                    va='bottom',
                    fontsize=8
                )

            # Add significance markers
            sig_pairs = df_overall_sig[(df_overall_sig['Metric'] == metric) &
                                       (df_overall_sig['Significant'])]

            if not sig_pairs.empty:
                y_max = df_metric['Upper'].max() * 1.1
                for idx, row in sig_pairs.iterrows():
                    modelA_idx = list(model_names.values()).index(row['ModelA'])
                    modelB_idx = list(model_names.values()).index(row['ModelB'])
                    plt.plot([modelA_idx, modelB_idx], [y_max, y_max], 'k-')
                    plt.text((modelA_idx + modelB_idx) / 2, y_max * 1.01, '*', ha='center')

            # Formatting
            plt.xticks(x_pos, [m.split(' ')[1] for m in model_names.values()], rotation=45)
            plt.title(metric)
            plt.ylabel('Average Score (Lower is Better)')

            if i >= 2:  # Add x-label to bottom plots only
                plt.xlabel('Model')

            plt.grid(axis='y', linestyle=':', alpha=0.4)

        plt.suptitle('Overall Model Performance with 95% Confidence Intervals', fontsize=16)
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.savefig(os.path.join(OUTPUT_DIR, 'overall_comparison_with_ci.png'), bbox_inches='tight', dpi=300)
        plt.savefig(os.path.join(OUTPUT_DIR, 'overall_comparison_with_ci.pdf'), bbox_inches='tight', dpi=300)
        plt.close()

        # Create ranking table for each metric
        ranking_data = {
            'Metric': list(metric_names.values()),
        }

        for model in models_list:
            model_key = model_names[model]
            scores = []
            cis = []
            for metric in metric_types:
                metric_fullname = metric_names[metric]
                df_row = df_overall_ci[(df_overall_ci['Model'] == model_key) &
                                       (df_overall_ci['Metric'] == metric_fullname)]
                if not df_row.empty:
                    mean_val = df_row.iloc[0]['Mean']
                    lower = df_row.iloc[0]['Lower']
                    upper = df_row.iloc[0]['Upper']
                    scores.append(mean_val)
                    cis.append(f"({lower:.4f}-{upper:.4f})")
                else:
                    scores.append(np.nan)
                    cis.append("")

            ranking_data[model_key] = scores
            ranking_data[f"{model_key} 95% CI"] = cis

        df_ranking = pd.DataFrame(ranking_data)

        # Add winner column (with statistical significance)
        winners = []
        for metric in metric_names.values():
            df_metric = df_overall_ci[df_overall_ci['Metric'] == metric]
            best_model = df_metric.loc[df_metric['Mean'].idxmin(), 'Model']

            # Check if significantly better than others
            sig_better_than_all = True
            for other_model in model_names.values():
                if other_model == best_model:
                    continue

                comparison = df_overall_sig[
                    (df_overall_sig['Metric'] == metric) &
                    (
                            ((df_overall_sig['ModelA'] == best_model) & (df_overall_sig['ModelB'] == other_model)) |
                            ((df_overall_sig['ModelA'] == other_model) & (df_overall_sig['ModelB'] == best_model))
                    )
                    ]

                if not comparison.empty:
                    if not comparison.iloc[0]['Significant']:
                        sig_better_than_all = False
                else:
                    sig_better_than_all = False

            winners.append(f"{best_model}{' *' if sig_better_than_all else ''}")

        df_ranking['Winner'] = winners

        print("\n--- Overall Model Ranking with Confidence Intervals (Test Set Evaluation) ---")
        print(df_ranking[['Metric'] + [m for m in model_names.values()] + ['Winner']].round(4).to_markdown(
            numalign="right", stralign="right"))

        # Save detailed results to CSV
        print("\nSaving detailed results to CSV files...")
        df_results.to_csv(os.path.join(OUTPUT_DIR, 'all_results.csv'), index=False)
        df_ci.to_csv(os.path.join(OUTPUT_DIR, 'binned_results_with_ci.csv'), index=False)
        df_overall_ci.to_csv(os.path.join(OUTPUT_DIR, 'overall_results_with_ci.csv'), index=False)
        df_significance.to_csv(os.path.join(OUTPUT_DIR, 'significance_tests.csv'), index=False)
        df_ranking.to_csv(os.path.join(OUTPUT_DIR, 'model_ranking_with_ci.csv'), index=False)

        print("\nEvaluation complete. Results and visualizations saved to the output directory.")
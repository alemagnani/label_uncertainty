\documentclass[journal]{IEEEtran}

\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}

% IEEE wants URLs in footnotes
\usepackage[hidelinks]{hyperref}

\title{Soft Metrics for Ordinal Label Uncertainty with Variable Sample Sizes}

% \author{Alessandro~Magnani
% \thanks{A. Magnani is with Coupang, Mountain View, CA, USA.
% Email: ale.magnani@gmail.com}}

\author{Anonymous Author(s)\\
Paper under double-blind review}


\begin{document}
\maketitle

\begin{abstract}
We address the challenge of handling uncertainty in ordinal label collections where the number of evaluations per instance varies significantly - from as few as two to hundreds of annotations. While this scenario is common in real-world applications like e-commerce ratings, content moderation, and medical diagnosis severity assessments, current approaches often ignore both the ordinal nature of labels and the varying degrees of uncertainty from inconsistent sample sizes. We propose two key extensions to existing soft metrics: First, we develop expectation-based versions of three fundamental soft metrics (KL divergence, cross-entropy, and Jensen-Shannon divergence) under a Dirichlet distribution assumption, deriving clean analytical solutions particularly for the KL divergence case. Second, we introduce Earth Mover's Distance as a soft metric to naturally capture the ordinal relationships between labels. Our approach is extensively validated through both theoretical examples and comprehensive experiments on real-world datasets, revealing that traditional empirical metrics and our uncertainty-aware measures can lead to different model rankings. Our framework provides a principled way to handle both label uncertainty and ordinality across various domains where human evaluation yields inconsistent numbers of annotations per instance. To facilitate research reproducibility and practical adoption, we provide an implementation of all proposed metrics and experimental code on GitHub.
\end{abstract}

\section{Introduction}
The collection of human annotations for machine learning tasks frequently results in datasets with ordinal labels - ratings or assessments that follow a natural order \cite{plank2022}. Examples abound across domains: e-commerce product ratings (1-5 stars), content moderation severity levels (benign to severely harmful), medical diagnosis classifications (from normal to severe), or educational assessment scores. While these labels are often treated as categorical variables for simplicity, this approach discards valuable ordinal information - a 4-star rating is not just different from a 2-star rating, it is better by a quantifiable amount.

A fundamental challenge in working with such datasets is the distinction between hard and soft labels. Hard labels assign a single categorical value to each instance, while soft labels maintain a probability distribution over possible values \cite{peterson2019}. While hard labels are simpler to work with computationally, they can be misleading when there is genuine uncertainty or disagreement among annotators \cite{uma2021}.

The challenge is compounded by the practical reality of data collection: the number of annotations per instance often varies dramatically within the same dataset. Some instances might have hundreds of ratings (popular products on e-commerce sites), while others might have as few as two (newly listed items). This variable sample size introduces different levels of uncertainty in our understanding of the true label distribution - we should be more confident in the average rating from 100 reviewers than from 2 reviewers \cite{uma2021}.

To address these challenges, we propose a principled framework that makes three key contributions:

\begin{enumerate}
    \item We develop expectation-based versions of fundamental soft metrics (KL divergence, cross-entropy, and Jensen-Shannon divergence) under a Dirichlet distribution assumption. This Bayesian approach naturally handles varying sample sizes by appropriately weighting the influence of prior knowledge versus observed data.
    
    \item We introduce Earth Mover's Distance (EMD) as a soft metric that explicitly captures ordinal relationships between labels. Unlike traditional metrics that treat categories as unrelated, EMD accounts for the distance between ordinal levels, providing more meaningful comparisons for ordinal data.
    
    \item We validate our approach through comprehensive experiments on both synthetic examples and two real-world datasets: Amazon Electronics Reviews \cite{he2016} and ConvAbuse \cite{cercas2021}. These experiments demonstrate that our uncertainty-aware metrics lead to systematically different model rankings compared to traditional approaches, particularly when annotation counts vary.
\end{enumerate}

Our framework offers several key advantages over existing approaches:

\begin{itemize}
    \item It naturally handles datasets with varying numbers of annotations per instance, appropriately adjusting confidence based on sample size
    \item It preserves and leverages ordinal relationships between labels, providing more meaningful metrics for ordinal data
    \item It provides well-calibrated uncertainty estimates that can be incorporated into downstream training procedures
    \item It maintains computational efficiency through analytical solutions and optimized approximations
\end{itemize}

Traditional approaches to handling label uncertainty have centered around three key soft metrics: Cross-entropy, KL divergence, and Jensen-Shannon divergence \cite{peterson2019}. These metrics allow us to compare predicted probability distributions with target distributions, but they typically assume the target distribution is known with certainty. In reality, when we have only a small sample of annotations, there is uncertainty in the target distribution itself \cite{pavlick2019}. Our work addresses this limitation by explicitly modeling the uncertainty in the target distribution while preserving ordinal relationships between labels.

To facilitate research reproducibility and adoption in practical applications, we provide a comprehensive implementation of all proposed metrics and experimental code on GitHub. This includes utilities for working with both the Amazon Electronics dataset and the ConvAbuse dataset, as well as implementations of the baseline models used in our experiments.

\section{Related Work}
\subsection{Human Label Variation in Machine Learning}
The challenge of human label variation has been increasingly recognized as a fundamental aspect of machine learning systems. \cite{plank2022} frames this not as a problem to be solved but as an inherent characteristic of human annotation that should be embraced. This perspective builds on earlier work by \cite{pavlick2019} who found that genuine disagreement between annotators is present in at least 20\% of their natural language inference dataset.

The sources of label variation are diverse. \cite{uma2021} categorize these into three main types: genuine disagreement due to inherent ambiguity, subjective differences in interpretation, and cases where multiple answers are plausible. This categorization has been influential in understanding why traditional approaches to achieving consensus might be inappropriate. \cite{aroyo2015} further challenged the common practice of treating disagreement as noise, introducing the concept of "crowd truth" to capture the inherent ambiguity in human interpretation.

\subsection{Approaches to Label Uncertainty}
Historical approaches to handling label uncertainty can be broadly categorized into two camps. The first aims to resolve uncertainty through aggregation or filtering. \cite{dawid1979} introduced a probabilistic approach to combining multiple annotations, which has been extensively developed in subsequent work \cite{paun2022}. However, \cite{peterson2019} demonstrated that attempting to resolve all uncertainty can actually make classification systems less robust.

The second camp embraces uncertainty as informative. \cite{sommerauer2020} proposed methods for working with "justified and informative disagreement," while \cite{fornaciari2021} developed techniques for learning directly from soft labels. This approach has gained traction particularly in areas with inherent subjectivity, such as emotion detection \cite{demszky2020} and content moderation.

\subsection{Statistical Approaches to Label Uncertainty}
The statistical treatment of label uncertainty has evolved significantly. \cite{smyth1995} pioneered early probabilistic approaches, but recent work has introduced more sophisticated methods. \cite{peterson2019} introduced human uncertainty modeling using Gaussian processes, while Bayesian approaches have been developed by several researchers \cite{gordon2021,li2019}.

Our work builds particularly on \cite{uma2021}'s framework for learning from disagreement, extending it to handle ordinal relationships and variable sample sizes. The use of Dirichlet distributions for modeling label uncertainty has precedent in the work of \cite{archambeau2016}, though our application to soft metrics is novel.

\subsection{Ordinal Labels in Machine Learning}
The special nature of ordinal labels has been recognized in various domains. \cite{cheplygina2018} demonstrated their importance in medical image annotation, while \cite{alm2011} explored their role in sentiment analysis. The traditional treatment of ordinal labels as categorical variables has been criticized by several researchers \cite{manning2011,zeman2010}.

Our approach to handling ordinal relationships using Earth Mover's Distance builds on work by \cite{levina2001} in the context of image comparison. However, our application to label distributions and integration with Bayesian uncertainty modeling represents a novel contribution.

\subsection{Applications in E-commerce and Content Moderation}
The practical importance of our work is particularly evident in e-commerce and content moderation settings. \cite{cercas2021} demonstrated the challenges of handling nuanced abuse detection annotations, while \cite{liu2016} explored similar issues in product rating systems. These domains frequently encounter both variable numbers of annotations and inherently ordinal labels, making them ideal applications for our methods.

\subsection{Approaches to Label Uncertainty}

The challenge of learning from uncertain or ambiguous labels has been approached from multiple perspectives. Peterson et al. \cite{peterson2019} proposed Joint-Distribution Soft Loss (JDSL), which directly models human uncertainty through soft label distributions rather than forcing consensus to hard labels. Their work shows that preserving disagreement information during training results in models that are more robust to annotation noise, though they don't specifically address varying sample sizes or ordinal relationships.

Learning from Label Proportions (LLP) \cite{quadrianto2009} presents a related approach where only the proportions of labels within groups are available. While originally developed for weakly supervised settings, these techniques can be adapted to scenarios where each item has multiple annotations. However, LLP methods typically don't account for the uncertainties introduced by small sample sizes.

The crowdsourcing literature offers several approaches to annotation uncertainty. The classic Dawid-Skene model \cite{dawid1979} and its extensions by Raykar et al. \cite{raykar2010} handle multiple annotators with varying reliability, but these methods focus primarily on estimating annotator quality rather than modeling the uncertainty from limited sample sizes. Paun et al. \cite{paun2018} provide a comprehensive comparison of aggregation methods for annotations, though they primarily target categorical rather than ordinal labels.

Bayesian approaches to label uncertainty have gained prominence recently. Kendall and Gal \cite{kendall2017} distinguish between aleatoric uncertainty (inherent data noise) and epistemic uncertainty (model uncertainty) in deep learning, with the former being particularly relevant to our problem of label noise. However, their approach doesn't specifically address the ordinal nature of labels.

Cui et al. \cite{cui2020} introduced Distribution Matching Networks that use Wasserstein distances for learning from frequency distributions, making them relevant to our use of Earth Mover's Distance for ordinal labels. Their work, however, doesn't incorporate Bayesian uncertainty quantification based on sample size.

Fornaciari et al. \cite{fornaciari2021} demonstrated the value of learning directly from disagreement distributions rather than aggregated labels, but did not address the variable confidence resulting from different sample sizes. Similarly, Uma et al. \cite{uma2021} proposed a framework for learning from disagreement but without the Bayesian calibration we propose.

For handling specifically ordinal labels, traditional approaches include ordered probit/logit models \cite{mccullagh1980}, while more recent machine learning approaches include Gaussian process ordinal regression \cite{chu2005} and ordinal distribution regression \cite{seeger2021}. These methods capture the ordinal relationships but typically don't incorporate uncertainty from variable annotation counts.

Our work differs from these approaches by simultaneously addressing both the ordinal nature of labels and the uncertainty arising from variable sample sizes within a unified Bayesian framework. While some previous approaches have addressed one aspect or the other, to our knowledge, no existing method provides the integrated treatment we propose.

\section{Methodology}

\subsection{Problem Formulation}
Consider a rating system with $K$ ordered labels $\{0,...,K-1\}$ where each label has a semantic interpretation (e.g., 0="poor", 1="fair", 2="good", 3="very good", 4="excellent"). For each item $i$, we collect $n_i$ ratings, where $n_i$ may vary across items.

\subsection{Bayesian Label Distribution Modeling}
Given a collection of labels with varying levels of annotator agreement and different sample sizes per instance, we need to model the inherent uncertainties in these labels. We approach this by formulating a Bayesian framework that explicitly models two key sources of uncertainty: the uncertainty in individual label assignments and the uncertainty in the underlying probability distribution of labels. This principled approach allows us to both incorporate prior knowledge and properly account for varying numbers of annotations per instance, while maintaining a clear understanding of our confidence in each label distribution.

\subsubsection{Prior and Posterior Distributions}
For a $K$-class labeling problem, we model the prior distribution over label probabilities $p = (p_1, ..., p_K)$ using a Dirichlet distribution:

\begin{equation}
p \sim \text{Dir}(\alpha_1, ..., \alpha_K)
\end{equation}

where $(\alpha_1, ..., \alpha_K)$ are the prior concentration parameters. A natural uninformative prior is the uniform Dirichlet with $\alpha_i = 1$ for all $i$, though the framework allows for informative priors when there is reason to expect certain labels to be more likely.

Given observed label counts $n = (n_1, ..., n_K)$, the posterior distribution is also Dirichlet due to conjugacy:

\begin{equation}
p|n \sim \text{Dir}(\alpha_1 + n_1, ..., \alpha_K + n_K)
\end{equation}

\subsubsection{Expected Label Probabilities}
The expected probability of label $i$ under this model is:

\begin{equation}
E[p_i|n] = \frac{\alpha_i + n_i}{\alpha_0 + n_0}
\end{equation}

where $\alpha_0 = \sum_i \alpha_i$ and $n_0 = \sum_i n_i$. This formula reveals an important correction to raw frequency estimates. For example, with a uniform prior ($\alpha_i = 1$) and two observed "excellent" ratings out of two total ratings, the expected probability of "excellent" would be:

\begin{equation}
E[p_\text{excellent}] = \frac{1 + 2}{K + 2}
\end{equation}

This is notably different from the raw frequency of 1.0, reflecting our uncertainty given the small sample size. The correction becomes less significant with larger sample sizes, as the posterior becomes dominated by the observed data.

\subsubsection{Expected Metrics}
Given this Bayesian formulation, we can derive expected versions of standard soft metrics. For a predicted distribution $q$, the expected cross-entropy is:

\begin{equation}
\begin{split}
E[\text{CrossEntropy}(p,q)] & = -\sum_i E[p_i|n]\log(q_i) \\
 & = -\sum_i \frac{\alpha_i + n_i}{\alpha_0 + n_0}\log(q_i)
\end{split}
\end{equation}

For comparing distributions, both KL divergence and cross-entropy lead to equivalent model rankings since they differ only by the entropy term $H(p)$, which is independent of the predicted distribution $q$. The expected KL divergence is:

\begin{equation}
\begin{split}
E[\text{KL}(p||q)] = \sum_i \frac{\alpha_i + n_i}{\alpha_0 + n_0} & (\psi(\alpha_i + n_i + 1) \\
 & - \psi(\alpha_0 + n_0 + 1) - \log(q_i))
\end{split}
\end{equation}

where $\psi(x)$ is the digamma function.

\subsubsection{Expected Metrics and Their Properties}
The derived expected metrics present several noteworthy characteristics:

\begin{enumerate}
\item \textbf{Computational Efficiency:} Despite their Bayesian foundation, these metrics require only minor modifications to standard implementations. The primary change involves replacing raw frequencies $\frac{n_i}{n_0}$ with their expected counterparts $\frac{\alpha_i + n_i}{\alpha_0 + n_0}$. This small adjustment can be implemented with minimal computational overhead, typically just a few additional floating-point operations per label.

\item \textbf{Analytical Tractability:} While Bayesian approaches often require numerical approximations, our formulation leads to clean analytical solutions, particularly for the KL divergence. The existence of closed-form expressions means we can compute these metrics exactly, without resort to Monte Carlo sampling or other approximation techniques.

\item \textbf{Smooth Uncertainty Transition:} The metrics naturally interpolate between prior knowledge and empirical observations. As sample size increases, the influence of the prior smoothly diminishes, providing a principled way to handle the spectrum from few to many annotations.

\item \textbf{Impact on Decision Boundaries:} While the mathematical modifications appear modest, they lead to substantially different behaviors in practice, particularly in cases with few annotations. As we will demonstrate in later examples, these differences can significantly affect model selection and evaluation decisions.

\item \textbf{Calibration Properties:} The expected metrics provide well-calibrated uncertainty estimates that reflect both the inherent randomness in the annotation process and our uncertainty about the true distribution. This calibration is particularly important for downstream tasks that rely on confidence estimates.
\end{enumerate}

These properties make our framework particularly suitable for real-world applications where annotation counts vary widely and reliable uncertainty estimates are crucial. The combination of theoretical elegance (analytical solutions), practical efficiency (minimal computational overhead), and improved behavior (better calibrated uncertainties) provides a compelling argument for adopting these metrics in practice.

Furthermore, the framework's flexibility allows for easy extension to incorporate domain-specific prior knowledge when available, while defaulting to uninformative priors when no such knowledge exists. This adaptability makes it suitable for a wide range of applications, from e-commerce ratings to medical diagnosis classifications.

\subsection{Earth Mover's Distance for Ordinal Labels}
The Earth Mover's Distance (EMD), also known as the Wasserstein metric, provides a natural way to capture the ordinal relationship between labels. Unlike traditional metrics that treat labels as categorical variables, EMD incorporates the "distance" that must be traveled to transform one distribution into another.

In our ordinal rating context with $K$ labels $\{0,...,K-1\}$, we define the ground distance between labels $i$ and $j$ as:

\begin{equation}
d(i,j) = \frac{|i - j|}{K-1}
\end{equation}

This normalization ensures distances are in $[0,1]$ while preserving the ordinal relationships. The EMD between two distributions $P$ and $Q$ is then defined as:

\begin{equation}
\text{EMD}(P,Q) = \min_{F} \sum_{i,j} f_{ij}d(i,j)
\end{equation}

subject to:
\begin{itemize}
\item $f_{ij} \geq 0$ (non-negative flow)
\item $\sum_j f_{ij} = P(i)$ (outflow constraint)
\item $\sum_i f_{ij} = Q(j)$ (inflow constraint)
\end{itemize}


\subsubsection{Expected EMD under Dirichlet Prior}
While we can analytically derive expectations for KL divergence and cross-entropy under a Dirichlet prior, the Earth Mover's Distance presents additional challenges. The expectation of EMD under a Dirichlet distribution:

\begin{equation}
E[\text{EMD}(p,q)] = \int \text{EMD}(p,q) \text{Dir}(p|\alpha) dp
\end{equation}

does not have a closed-form solution due to the optimization problem embedded in the EMD computation. However, we can approximate this expectation through numerical integration:

\begin{equation}
E[\text{EMD}(p,q)] \approx \frac{1}{N} \sum_{i=1}^N \text{EMD}(p^{(i)},q)
\end{equation}

where $p^{(i)}$ are samples drawn from the Dirichlet distribution. This Monte Carlo approximation converges to the true expectation as $N \to \infty$. In practice, we find that moderate values of $N$ (e.g., 1000) provide stable estimates.

\subsubsection{Computational Complexity Considerations}
The computational complexity of EMD and its expected variant requires careful consideration for practical applications. For the standard EMD between two distributions:

\begin{itemize}
\item Basic EMD computation: $O(K^3 \log K)$ using the transportation simplex algorithm, where $K$ is the number of labels
\item For 1D ordered labels (our case): Reduces to $O(K)$ by computing cumulative distribution differences
\item Memory requirement: $O(K^2)$ for the cost matrix
\end{itemize}

For the expected EMD:
\begin{itemize}
\item Monte Carlo approximation with $N$ samples: $O(NK)$ for our 1D case
\item Each sample requires drawing from Dirichlet: $O(K)$
\item Total complexity: $O(NK)$ where $N$ is the number of Monte Carlo samples
\end{itemize}

In practice, we can optimize performance by:
\begin{itemize}
\item Pre-computing cost matrices for fixed label sets
\item Using parallel computation for Monte Carlo samples
\item Caching Dirichlet samples for repeated computations with the same parameters
\end{itemize}

For real-time applications where computation speed is critical, we can further approximate using smaller $N$ or pre-computed expected values for common parameter configurations.

\subsection{Illustrative Examples}

\subsubsection{Example 1: Impact of Sample Size}
Consider a 5-star rating system where we want to understand how the number of ratings affects our certainty in the distribution and how this impacts prediction evaluation. We examine a case where:

\begin{itemize}
\item Initial observations: [1,2,4,2,1] ratings (10 total, centered at 2 stars)
\item Prior: Dirichlet($\alpha = [1,1,1,1,1]$) (uniform prior)
\item Two predictive distributions:
  \begin{itemize}
    \item Matching: q = [0.1, 0.2, 0.4, 0.2, 0.1] (aligns with data)
    \item Mismatched: q = [0.1, 0.3, 0.3, 0.2, 0.1] (predicts lower ratings)
  \end{itemize}
\end{itemize}

Figure \ref{fig:cross_entropy} shows how the expected cross-entropy changes as we increase the number of observations by factors of 1 (10 ratings), 2 (20 ratings), 4 (40 ratings), and 10 (100 ratings). The expected cross-entropy is calculated as:

\begin{equation}
\begin{split}
E[\text{CrossEntropy}(p,q)] & = -\sum_{i=1}^K E[p_i]\log(q_i) \\
 & = -\sum_{i=1}^K \frac{\alpha_i + n_i}{\alpha_0 + n_0}\log(q_i)
\end{split}
\end{equation}

where $\alpha_i$ are the prior parameters, $n_i$ are the observed counts, and $q_i$ is the predicted probability for category $i$.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{cross_entropy_plot.pdf}
\caption{Expected cross-entropy for matching and mismatched predictions as observations increase. The plot shows two key effects: (1) cross-entropy decreases with more observations for both predictions as uncertainty reduces, and (2) the gap between good and poor predictions widens with more data, showing increased discriminative power.}
\label{fig:cross_entropy}
\end{figure}

The plot reveals several key insights about uncertainty in ordinal label collections:

\begin{enumerate}
\item With few observations (n=10), cross-entropy is higher for both predictions due to uncertainty, and the difference between good and poor predictions is less pronounced.
\item As observations increase, cross-entropy decreases for both predictions as the uncertainty from the prior diminishes.
\item The gap between matching and mismatched predictions widens with more observations, showing that the metric becomes more discriminative with more data.
\end{enumerate}

This example illustrates how our framework naturally handles varying sample sizes. The Dirichlet formulation provides appropriate uncertainty estimates that make the metric more conservative with few ratings (where the prior has more influence) and more discriminative with many ratings (where the empirical distribution dominates). This behavior aligns with our intuition that we should be less certain about true preferences when we have fewer ratings.


\subsubsection{Example 2: Ordinal vs Categorical Differences}
Consider two scenarios with the same KL divergence but different ordinal implications:

Scenario A:
\begin{itemize}
\item True distribution: [0.0, 0.8, 0.2, 0.0, 0.0]
\item Predicted distribution: [0.0, 0.2, 0.8, 0.0, 0.0]
\item Error: adjacent label confusion
\end{itemize}

Scenario B:
\begin{itemize}
\item True distribution: [0.0, 0.8, 0.0, 0.0, 0.2]
\item Predicted distribution: [0.0, 0.2, 0.0, 0.0, 0.8]
\item Error: distant label confusion
\end{itemize}

Traditional metrics:
\begin{itemize}
\item $\text{KL}(A) \approx \text{KL}(B) \approx 0.97$
\item $\text{CrossEntropy}(A) \approx \text{CrossEntropy}(B) \approx 1.83$
\end{itemize}

Our EMD metric captures the ordinal difference:
\begin{itemize}
\item $\text{EMD}(A) \approx 0.15$ (small ordinal error)
\item $\text{EMD}(B) \approx 0.45$ (large ordinal error)
\end{itemize}

\section{Experimental Validation on Amazon Electronics Reviews Dataset}

To validate our theoretical framework in a real-world setting, we conducted experiments using the Amazon Electronics Reviews dataset \cite{he2016}. This experiment provides empirical evidence for our two key contributions: properly accounting for sample size uncertainty and preserving ordinal relationships between labels.

\subsection{Experimental Setup}

We sampled 20,000 products with at least 5 reviews each, using product titles as features for predicting rating distributions. Products exhibited natural variation in review counts, ranging from as few as 5 reviews to over 100 per product. We implemented and compared four distinct models:

\begin{itemize}
    \item \textbf{Model A (MLP+CE)}: A standard multilayer perceptron trained with categorical cross-entropy loss against raw empirical distributions.
    
    \item \textbf{Model B (MLP+KL)}: Identical architecture to Model A, but trained with KL divergence loss on Gaussian-smoothed target distributions, implicitly respecting ordinal relationships.
    
    \item \textbf{Model C (BONN)}: A Bayesian Ordinal Neural Network explicitly modeling the ordinal nature of ratings through a cumulative link approach.
    
    \item \textbf{Model D (TORP)}: A transformer-based architecture trained with EMD-aware loss, representing a more complex approach.
\end{itemize}

\begin{table}[t]
\centering
\small
\caption{Overall model ranking by metric type (lower scores are better, best scores in \textbf{bold})}
\label{tab:model_ranking}
\begin{tabular}{|l|cccc|}
\hline
\textbf{Metric} & \textbf{Model A} & \textbf{Model B} & \textbf{Model C} & \textbf{Model D} \\
\hline
CE Empirical & \textbf{1.251} & 1.255 & 1.268 & 1.265 \\
E[CE] & 1.455 & \textbf{1.443} & 1.451 & 1.447 \\
EMD Empirical & \textbf{0.531} & 0.536 & 0.544 & 0.534 \\
E[EMD] & 0.542 & \textbf{0.526} & 0.542 & 0.532 \\
\hline
\end{tabular}
\end{table}

\subsection{Results and Analysis}

Our experimental results reveal three key findings that support our theoretical framework:

\subsubsection{Metric Choice Determines Model Selection}

The most striking result, shown in Table~\ref{tab:model_ranking}, is how the choice of evaluation metric dramatically affects which model appears superior. Under traditional empirical metrics (CE\_emp and EMD\_emp), the simple MLP with standard cross-entropy loss (Model A) outperforms all others. However, when evaluated with expected metrics that incorporate uncertainty, Model B emerges as the clear winner.

This model ranking reversal confirms our central claim: failing to account for label uncertainty leads to systematically different evaluation outcomes. The smoothed-target approach used in Model B implicitly respects both uncertainty and ordinal relationships, explaining its superior performance under metrics that value these properties.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{model_comparison_E_EMD.png}
    \caption{Expected Earth Mover's Distance (E[EMD]) across review count bins. Model B outperforms other models in bins with fewer reviews, demonstrating the value of uncertainty modeling in small-sample scenarios.}
    \label{fig:emd_bin_comparison}
\end{figure}

\subsubsection{Uncertainty Calibration by Sample Size}

Figure~\ref{fig:emd_bin_comparison} demonstrates how model performance varies with sample size when evaluated using E[EMD]. For products with minimal reviews (5-5 bin), Model B shows a clear advantage over other approaches. As review counts increase, this advantage diminishes, and by the 101+ bin, the gap becomes minimal.

This pattern directly confirms our theoretical prediction: uncertainty modeling matters most when sample sizes are small. The fact that Model B's advantage is most pronounced in the 5-5 bin validates our framework's emphasis on properly calibrating confidence based on available evidence.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{model_comparison_CE_emp.png}
    \caption{CE Empirical scores across review count bins reveal an intriguing pattern: performance worsens in middle bins before improving again, reflecting the challenges of reconciling empirical distributions from different sample sizes.}
    \label{fig:ce_empirical_curve}
\end{figure}

\subsubsection{The Paradox of Empirical Metrics}

An unexpected finding shown in Figure~\ref{fig:ce_empirical_curve} is the non-monotonic relationship between review count and empirical CE performance. For all models, performance initially worsens as review counts increase from 5 to 20, then improves again for products with more reviews. This counterintuitive pattern highlights a fundamental challenge with empirical metrics: they don't account for the higher variance in small samples.

When a product has only 5 reviews, empirical metrics "reward" models for matching the observed distribution exactly, even though this distribution is likely a poor estimate of the true underlying preferences. Our expected metrics avoid this pitfall by appropriately discounting noisy empirical distributions from small samples.

The fact that sophisticated architectures (Models C and D) don't outperform the simpler Model B when evaluated with uncertainty-aware metrics indicates that the training objective—specifically, using KL divergence with smoothed targets—may be more important than architectural complexity when handling ordinal label uncertainty.

\subsection{Practical Implications}

These results have significant practical implications for e-commerce platforms and other applications involving ordinal ratings:

\begin{enumerate}
    \item \textbf{Model Selection Impact}: Using expected metrics leads to selecting different models than traditional empirical metrics would suggest, potentially improving performance on new items with few ratings.
    
    \item \textbf{Training Strategy}: Smoothing target distributions during training (as in Model B) effectively captures ordinal relationships without requiring complex architectures.
    
    \item \textbf{Sample Size Adaptation}: Expected metrics naturally adjust confidence based on available evidence, providing more reliable evaluations across different sample size regimes.
\end{enumerate}

The model ranking reversal between empirical and expected metrics confirms that traditional evaluation approaches may systematically undervalue models that handle uncertainty appropriately. For real-world recommendation systems dealing with variable review counts, our framework provides a principled approach to both training and evaluation that respects the confidence we should place in distributions from different sample sizes.

These experimental results on real-world data validate both key contributions of our work: properly accounting for varying sample sizes and preserving ordinal relationships between labels.


\section{Experimental Validation on ConvAbuse Dataset}

To further validate our framework in a different domain, we conducted experiments using the ConvAbuse dataset \cite{cercas2021}, which contains annotated conversations between users and three conversational AI systems. This experiment provides empirical evidence for our approach's effectiveness in handling both label uncertainty and ordinal relationships in a conversational abuse detection context.

\subsection{Dataset Characteristics}

The ConvAbuse dataset comprises conversations from three different conversational systems: an open-domain social bot (Alana v2), a rule-based chatbot (ELIZA), and a task-based system (CarbonBot). The dataset includes fine-grained annotations of abuse severity on an ordinal scale from +1 (non-abusive) to -3 (strongly abusive), provided by multiple expert annotators with backgrounds in gender studies. Unlike the Amazon Electronics dataset where uncertainty stems from naturally varying numbers of product ratings, the ConvAbuse dataset features a more controlled annotation environment with each instance typically having 2-5 expert annotations.

The dataset's ordinal nature and expert annotations provide an ideal test case for our metrics that are designed to handle ordinal relationships while accounting for annotation uncertainty.

\subsection{Experimental Setup}

For this experiment, we implemented two BERT-based models:

\begin{itemize}
    \item \textbf{Model A (No Context)}: A model trained on individual user utterances without considering the surrounding conversational context.
    
    \item \textbf{Model B (With Context)}: An identical architecture trained with conversational context, including previous turns from both the user and the system.
\end{itemize}

We applied our Dirichlet-based uncertainty handling approach to appropriately weight the annotations, setting uniform priors with $\alpha = [1,1,1,1,1]$ as in our previous experiments. We evaluated both models using traditional empirical metrics (CE and EMD calculated on raw annotation frequencies) and our expected metrics that incorporate uncertainty (E[CE] and E[EMD]).

\begin{table}[t]
\centering
\caption{Overall model performance on ConvAbuse dataset (lower scores are better, best scores in \textbf{bold})}
\label{tab:convabuse_results}
\begin{tabular}{|l|cc|c|}
\hline
\textbf{Metric} & \textbf{Model A} & \textbf{Model B} & \textbf{Winner} \\
\hline
CE Empirical & \textbf{0.473} & 0.484 & Model A \\
E[CE] & 2.580 & \textbf{2.526} & Model B \\
EMD Empirical & \textbf{0.251} & 0.276 & Model A \\
E[EMD] & 1.125 & \textbf{1.108} & Model B \\
\hline
\end{tabular}
\end{table}

\subsection{Results and Analysis}

Our experimental results, summarized in Table \ref{tab:convabuse_results}, reveal a striking pattern that aligns with our findings from the Amazon Electronics dataset. When evaluated using traditional empirical metrics, Model A (without context) appears to outperform Model B. However, when our expected metrics that incorporate uncertainty are applied, the ranking is reversed, with Model B (with context) emerging as the superior model.

This reversal in model ranking provides further evidence for our central thesis that traditional metrics may systematically undervalue models that handle uncertainty appropriately. The contextual information utilized by Model B provides valuable signals for abuse detection, but this benefit is only apparent when uncertainty is properly modeled using our expected metrics.

\begin{table}[t]
\centering
\caption{Average scores per annotation count bin (lower is better)}
\label{tab:convabuse_bins}
\small
\begin{tabular}{|c|c|cc|cc|}
\hline
\textbf{n\_bin} & \textbf{Count} & \multicolumn{2}{c|}{\textbf{CE\_emp}} & \multicolumn{2}{c|}{\textbf{E[CE]}} \\
 & & \textbf{A} & \textbf{B} & \textbf{A} & \textbf{B} \\
\hline
2-3 & 508 & \textbf{0.498} & 0.513 & 2.649 & \textbf{2.588} \\
4-5 & 93 & 0.358 & \textbf{0.351} & 2.248 & \textbf{2.229} \\
6-10 & 8 & \textbf{0.199} & 0.202 & 2.055 & \textbf{2.047} \\
\hline
\end{tabular}
\end{table}

To better understand the effect of annotation count on model performance, we analyzed results across different bins of annotation counts, as shown in Table \ref{tab:convabuse_bins}. The analysis reveals several key patterns:

\begin{enumerate}
    \item \textbf{Decreasing metric values with increasing annotation counts}: Across all metrics, scores decrease (improve) as the number of annotations increases, reflecting lower uncertainty with more annotator consensus.
    
    \item \textbf{Model B's advantage in expected metrics}: In instances with few annotations (2-3), Model B shows a clear advantage under the expected metrics, despite being outperformed by Model A under empirical metrics for the same instances.
    
    \item \textbf{Converging performance with more annotations}: As annotation counts increase, the performance gap between the two models narrows, suggesting that with sufficient annotations, even empirical metrics begin to capture the benefits of contextual information.
\end{enumerate}

The detailed bin-by-bin analysis shows that the majority of the test data (508 out of 609 instances) falls into the 2-3 annotation bin, highlighting the prevalence of scenarios with minimal annotations in real-world applications. This underscores the importance of methods that can handle uncertainty from limited annotations.

\subsection{Discussion}

These results on the ConvAbuse dataset complement our findings from the Amazon Electronics experiments in several important ways:

\begin{itemize}
    \item \textbf{Domain Generalization}: The consistent pattern of model ranking reversal between empirical and expected metrics across two vastly different domains—e-commerce product ratings and conversational abuse detection—demonstrates the broad applicability of our approach.
    
    \item \textbf{Annotation Paradigm Transfer}: While the Amazon dataset featured naturally occurring ratings with highly variable counts (from a few to hundreds), ConvAbuse represents a controlled annotation environment with expert judgments (typically 2-5 per instance). Our framework proves effective in both paradigms.
    
    \item \textbf{Value of Contextual Information}: The ConvAbuse results suggest that conversational context provides substantial information for abuse detection, but this benefit is only apparent when uncertainty is properly modeled using our expected metrics. This demonstrates how inappropriate uncertainty handling can mask genuine model improvements.
    
    \item \textbf{Practical Implications}: In abuse detection scenarios where obtaining large numbers of annotations is costly or impractical, our expected metrics provide a more reliable evaluation framework that correctly identifies superior models even with minimal annotations.
\end{itemize}

The Earth Mover's Distance metrics successfully capture the ordinal nature of abuse severity ratings, with E[EMD] providing a clear differentiation between models that aligns with our expected result that including context should improve abuse detection. This further validates our approach to handling uncertain ordinal labels, showing its effectiveness in the important application area of conversational abuse detection.


\section{Derivation of Expected Entropy}

Under the Dirichlet distribution, the expectation becomes:

\begin{equation}
E[p_i \log p_i] = \int p_i \log p_i \frac{1}{B(\alpha)} \prod_{j=1}^K p_j^{\alpha_j-1} dp
\end{equation}

where $B(\alpha)$ is the multivariate beta function.

This integral can be solved by recognizing that it's related to the derivative of the log normalization constant with respect to $\alpha_i$. The key insight is:

\begin{equation}
E[p_i \log p_i] = \frac{\partial}{\partial \alpha_i} \log B(\alpha)
\end{equation}

The derivative of the log multivariate beta function can be expressed in terms of the digamma function:

\begin{equation}
\frac{\partial}{\partial \alpha_i} \log B(\alpha) = \frac{\alpha_i}{\alpha_0}(\psi(\alpha_i + 1) - \psi(\alpha_0 + 1))
\end{equation}
where $\psi(x)$ is the digamma function and $\alpha_0 = \sum_{i=1}^K \alpha_i$.

Therefore, the expected entropy is:

\begin{equation}
E[H(p)] = -\sum_{i=1}^K \frac{\alpha_i}{\alpha_0}(\psi(\alpha_i + 1) - \psi(\alpha_0 + 1))
\end{equation}

\subsection{Properties of the Result}
This result has several important properties:
\begin{itemize}
\item The expected entropy depends on both individual $\alpha_i$ values and their sum
\item As $\alpha_0 \to \infty$ with fixed ratios, it converges to the entropy of the expected distribution
\item When all $\alpha_i = 1$ (uniform prior), we get maximum uncertainty
\item The expression maintains the non-negativity of entropy
\item The result is symmetric in the parameters, respecting the exchangeability of the Dirichlet distribution
\end{itemize}

\section{Conclusions}

This paper addresses a fundamental challenge in modern machine learning systems: the principled handling of human-annotated ordinal data with varying levels of confidence. Our framework makes three key theoretical contributions - expectation-based soft metrics, ordinal-aware distance measures, and analytical solutions for uncertainty quantification - that directly translate into practical improvements for real-world applications.

The importance of this work is particularly evident in industrial settings:

\begin{itemize}
\item \textbf{E-commerce Systems:} Product rating predictions can now account for both the number of reviews and their ordinal nature, leading to more reliable recommendations for newly listed items with few ratings while maintaining accuracy for popular products with hundreds of reviews.

\item \textbf{Content Moderation:} Our framework enables more nuanced handling of severity assessments, appropriately weighting decisions from expert moderators versus crowd workers, while respecting the ordinal nature of violation categories.

\item \textbf{Medical Diagnostics:} The approach naturally handles varying levels of confidence in disease severity assessments, whether from a single specialist or a panel of experts, maintaining appropriate uncertainty estimates that are crucial for clinical decision-making.
\end{itemize}

Beyond specific applications, our framework addresses several systemic issues in current machine learning pipelines. First, it eliminates the false dichotomy between hard and soft labels, providing a principled way to handle all levels of annotation confidence. Second, it preserves valuable ordinal information that is typically discarded when treating ratings as categorical variables. Third, it provides well-calibrated uncertainty estimates that can be propagated through the entire machine learning pipeline.

The computational efficiency of our approach - requiring only minor modifications to existing metrics while providing substantial improvements in uncertainty handling - makes it particularly attractive for industrial adoption. Our analytical solutions and efficient approximations ensure that these benefits come without significant computational overhead, making the framework suitable for both batch processing and real-time applications.

Our comprehensive experimental validation on synthetic examples and two real-world datasets reveals a consistent pattern: traditional empirical metrics and our uncertainty-aware measures can lead to systematically different model rankings. This finding has profound implications for model selection and evaluation in domains where human annotation plays a central role.

The availability of our implementation and experimental code on GitHub facilitates both research reproducibility and practical adoption of these methods in production environments. We provide utilities for working with both the Amazon Electronics dataset and the ConvAbuse dataset, enabling researchers and practitioners to easily apply our framework to their own annotation challenges.

Looking forward, this work opens several promising directions for future research. The framework could be extended to handle hierarchical label structures, incorporate domain-specific prior knowledge, and integrate with active learning systems. Additionally, the principles developed here could be applied to other types of structured labels where confidence varies by instance.

This work also highlights the broader opportunity in treating human label variation not as noise to be eliminated, but as valuable signal to be understood and leveraged. In an era where machine learning systems increasingly interact with human judgment, frameworks that can appropriately handle varying levels of human certainty become crucial for building trustworthy systems.

In conclusion, while our technical contributions advance the theoretical understanding of ordinal label uncertainty, their real value lies in enabling more reliable and nuanced machine learning systems in production environments where human judgment plays a crucial role. As these systems become increasingly prevalent in critical applications, frameworks that can appropriately handle human label variation while maintaining computational efficiency become not just theoretically interesting, but practically essential.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
